{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPbDKehW4qCcF8V919FoZ5W"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"XAa30Jb0LjMg"},"outputs":[],"source":["!pip install transformers --upgrade"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"T1R-P7DfMDt1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install accelerate\n","!pip install -i https://pypi.org/simple/ bitsandbytes\n","# !pip install accelerate bitsandbytes\n","# !pip install --upgrade accelerate\n","# !pip install -U bitsandbytes-cuda111\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","\n","# to use 4bit use `load_in_4bit=True` instead\n","quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n","\n","checkpoint = \"bigcode/starcoder2-3b\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = AutoModelForCausalLM.from_pretrained(checkpoint, quantization_config=quantization_config)"],"metadata":{"id":"0MZTDYuUMEW5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CodingProblem:\n","    def __init__(self, problem_statement, solution):\n","        self.problem_statement = problem_statement\n","        self.solution = solution"],"metadata":{"id":"6onhIqP9MHbp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","\n","def load_format_1(file_path):\n","    problems = []\n","    with open(file_path, 'r') as file:\n","        data_list = json.load(file)\n","        for data in data_list:\n","            problem_statement = data.get(\"instruction\", \"\") + \" \" + data.get(\"input\", \"\")\n","            solution = data.get(\"output\", \"\")\n","            problem = CodingProblem(problem_statement=problem_statement, solution=solution)\n","            problems.append(problem)\n","    return problems"],"metadata":{"id":"ug3pMHrMMK3F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_path1 = '/content/drive/MyDrive/StarCoder/tigerbot-kaggle-leetcodesolutions-en-2k.json'\n","problems = load_format_1(file_path1)"],"metadata":{"id":"VcME-aZOMNHu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_format_2(file_path):\n","    new_problems = []\n","    with open(file_path, 'r') as file:\n","        data_list = json.load(file)  # Assuming this is also a list of entries\n","        for data in data_list:\n","            # Concatenate relevant parts to form a problem statement\n","            problem_statement = data.get(\"title\", \"\") + \" \" + data.get(\"algo_input\", \"\")\n","            # Choose one of the solutions. Here, I'm arbitrarily choosing the Python solution.\n","            solution = data.get(\"solution_py\", \"\") + \" \" + data.get(\"solution_js\", \"\") + \" \" + data.get(\"solution_java\", \"\") + \" \" + data.get(\"solution_c\", \"\")\n","            problem = CodingProblem(problem_statement=problem_statement, solution=solution)\n","            new_problems.append(problem)\n","    return new_problems"],"metadata":{"id":"6lRmuWWUMZLB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_path2 = '/content/drive/MyDrive/StarCoder/train.json'\n","new_problems = load_format_2(file_path2)\n","\n","# Extend the original problems list with the new problems\n","problems.extend(new_problems)"],"metadata":{"id":"9hrZsMpmMbt0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_path3 = '/content/drive/MyDrive/StarCoder/evaluation.json'\n","new_problems1 = load_format_2(file_path3)\n","\n","# Extend the original problems list with the new problems\n","problems.extend(new_problems1)"],"metadata":{"id":"LtQqTTA5M7hR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","\n","class CodingProblemsDataset(Dataset):\n","    def __init__(self, problems, tokenizer, max_length=512):\n","        self.problems = problems\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.problems)\n","\n","    def __getitem__(self, idx):\n","        problem = self.problems[idx]\n","        encoding = self.tokenizer(\n","            f\"Problem: {problem.problem_statement} Solution: {problem.solution}\",\n","            truncation=True,\n","            padding='max_length',\n","            max_length=self.max_length,\n","            return_tensors='pt'\n","        )\n","        encoding = {key: val.squeeze(0) for key, val in encoding.items()}  # Remove the batch dimension\n","        return encoding\n"],"metadata":{"id":"_RjXOJrBdVBf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","# Instantiate your dataset\n","dataset = CodingProblemsDataset(problems, tokenizer, max_length=512)\n","\n","# Use DataLoader to handle batching and memory management efficiently\n","data_loader = DataLoader(dataset, batch_size=4, shuffle=True)  # Adjust batch_size based on your RAM\n"],"metadata":{"id":"i4ie3rcLdXSg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set the padding token to `eos_token` if `pad_token` is not defined\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","# Proceed with your data generation and training loop\n"],"metadata":{"id":"Awn-EcR8dyMe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def data_generator(problems, tokenizer, batch_size=4, max_length=512):\n","    for i in range(0, len(problems), batch_size):\n","        batch_problems = problems[i:i + batch_size]\n","        batch_encodings = tokenizer(\n","            [f\"Problem: {p.problem_statement} Solution: {p.solution}\" for p in batch_problems],\n","            truncation=True,\n","            padding=True,\n","            max_length=max_length,\n","            return_tensors='pt'\n","        )\n","        yield batch_encodings\n","\n","# Use the generator during training\n","for batch_encodings in data_generator(problems, tokenizer):\n","    # Move tensors to GPU\n","    batch_encodings = {k: v.to('cuda') for k, v in batch_encodings.items()}\n","    # Continue with your training step\n"],"metadata":{"id":"_Xb-j60RdboS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"IstA5drgM0sH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import json\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, AutoModelForCausalLM, AdamW, get_linear_schedule_with_warmup\n","\n","# Ensure GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define the CodingProblem class and data loading functions (load_format_1 and load_format_2) here...\n","\n","# Load and combine data from multiple sources\n","file_path1 = '/content/drive/MyDrive/StarCoder/tigerbot-kaggle-leetcodesolutions-en-2k.json'\n","problems_format_1 = load_format_1(file_path1)\n","\n","file_path2 = '/content/drive/MyDrive/StarCoder/train.json'\n","problems_format_2 = load_format_2(file_path2)\n","\n","file_path3 = '/content/drive/MyDrive/StarCoder/evaluation.json'\n","problems_format_3 = load_format_2(file_path3)\n","\n","all_problems = problems_format_1 + problems_format_2 + problems_format_3\n","\n","# Load tokenizer and model\n","checkpoint = \"bigcode/starcoder2-3b\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n","\n","# If tokenizer does not have a pad token, set it to eos_token\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","# Create the dataset and DataLoader using the combined list of problems\n","dataset = CodingProblemsDataset(all_problems, tokenizer, max_length=512)\n","data_loader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=4, pin_memory=True)\n","\n","# Setup optimizer and scheduler (define num_epochs and other hyperparameters as needed)\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(data_loader) * num_epochs)\n","\n","import gc\n","\n","# Clear unused variables\n","del problems_format_1, problems_format_2, problems_format_3, all_problems\n","gc.collect()  # Python's garbage collector\n","\n","# PyTorch's cache clearing (helpful if moving tensors between CPU and GPU)\n","torch.cuda.empty_cache()\n","\n","\n","# Training loop\n","model.train()\n","for epoch in range(num_epochs):\n","    for batch in data_loader:\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        outputs = model(**batch, labels=batch['input_ids'])\n","        loss = outputs.loss\n","\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","\n","        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n","\n","# Save the model\n","model.save_pretrained('/content/drive/MyDrive/StarCoder')\n"],"metadata":{"id":"CfmQu0xseupH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the path where your fine-tuned model and tokenizer are saved\n","model_path = '/content/drive/MyDrive/StarCoder'\n","\n","# Load the fine-tuned model and tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_path)\n","model = AutoModelForCausalLM.from_pretrained(model_path)\n","\n","# Ensure the model is in evaluation mode\n","model.eval()\n","\n","# Function to generate text using the model\n","def generate_text(prompt, max_length=50):\n","    # Tokenize the prompt text\n","    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n","\n","    # Generate text using the model\n","    with torch.no_grad():\n","        output_ids = model.generate(input_ids, max_length=max_length, num_beams=5, early_stopping=True)[0]\n","\n","    # Decode the generated text\n","    generated_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n","\n","    return generated_text\n","\n","# Example usage\n","prompt = \"Define a function in Python that calculates the sum of two numbers:\"\n","\n","# Generate text\n","generated_solution = generate_text(prompt)\n","\n","print(\"Generated Solution:\")\n","print(generated_solution)"],"metadata":{"id":"xXU7CzALgTC6"},"execution_count":null,"outputs":[]}]}